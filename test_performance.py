#!/usr/bin/env python3
"""
æ€§èƒ½æµ‹è¯•å’Œå¯¹æ¯”
"""
import time
import requests
import json
from openai import OpenAI

def test_streaming_performance():
    """æµ‹è¯•æµå¼å“åº”çš„æ€§èƒ½"""
    print("ğŸš€ æµå¼å“åº”æ€§èƒ½æµ‹è¯•...\n")
    
    client = OpenAI(
        base_url="http://127.0.0.1:8000/v1",
        api_key="dummy"
    )
    
    test_cases = [
        {
            "name": "çŸ­å“åº” (20 tokens)",
            "messages": [{"role": "user", "content": "Count from 1 to 5"}],
            "max_tokens": 20
        },
        {
            "name": "ä¸­ç­‰å“åº” (50 tokens)", 
            "messages": [{"role": "user", "content": "Explain what is machine learning in simple terms"}],
            "max_tokens": 50
        },
        {
            "name": "é•¿å“åº” (100 tokens)",
            "messages": [{"role": "user", "content": "Write a short story about a robot learning to cook"}],
            "max_tokens": 100
        }
    ]
    
    for test_case in test_cases:
        print(f"ğŸ“Š æµ‹è¯•: {test_case['name']}")
        
        # æµ‹è¯•æµå¼å“åº”
        start_time = time.time()
        first_token_time = None
        tokens_received = 0
        
        try:
            stream = client.chat.completions.create(
                model="Qwen/Qwen3-8B-AWQ",
                messages=test_case["messages"],
                max_tokens=test_case["max_tokens"],
                temperature=0.7,
                stream=True
            )
            
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    if first_token_time is None:
                        first_token_time = time.time()
                    tokens_received += 1
            
            end_time = time.time()
            
            total_time = end_time - start_time
            first_token_latency = first_token_time - start_time if first_token_time else 0
            tokens_per_second = tokens_received / total_time if total_time > 0 else 0
            
            print(f"  âœ… æ€»æ—¶é—´: {total_time:.2f}s")
            print(f"  âš¡ é¦–Tokenå»¶è¿Ÿ: {first_token_latency:.2f}s")
            print(f"  ğŸ“ˆ æ¥æ”¶tokens: {tokens_received}")
            print(f"  ğŸ”¥ ç”Ÿæˆé€Ÿåº¦: {tokens_per_second:.2f} tokens/s")
            
        except Exception as e:
            print(f"  âŒ é”™è¯¯: {e}")
        
        print()

def test_non_streaming_performance():
    """æµ‹è¯•éæµå¼å“åº”çš„æ€§èƒ½"""
    print("ğŸ“¦ éæµå¼å“åº”æ€§èƒ½æµ‹è¯•...\n")
    
    client = OpenAI(
        base_url="http://127.0.0.1:8000/v1",
        api_key="dummy"
    )
    
    test_cases = [
        {
            "name": "çŸ­å“åº” (20 tokens)",
            "messages": [{"role": "user", "content": "What is 2+2?"}],
            "max_tokens": 20
        },
        {
            "name": "ä¸­ç­‰å“åº” (50 tokens)",
            "messages": [{"role": "user", "content": "Explain what is Python programming"}],
            "max_tokens": 50
        }
    ]
    
    for test_case in test_cases:
        print(f"ğŸ“Š æµ‹è¯•: {test_case['name']}")
        
        start_time = time.time()
        
        try:
            response = client.chat.completions.create(
                model="Qwen/Qwen3-8B-AWQ",
                messages=test_case["messages"],
                max_tokens=test_case["max_tokens"],
                temperature=0.7
            )
            
            end_time = time.time()
            total_time = end_time - start_time
            
            completion_tokens = response.usage.completion_tokens
            tokens_per_second = completion_tokens / total_time if total_time > 0 else 0
            
            print(f"  âœ… æ€»æ—¶é—´: {total_time:.2f}s")
            print(f"  ğŸ“ˆ ç”Ÿæˆtokens: {completion_tokens}")
            print(f"  ğŸ”¥ ç”Ÿæˆé€Ÿåº¦: {tokens_per_second:.2f} tokens/s")
            print(f"  ğŸ“ å“åº”: {response.choices[0].message.content[:100]}...")
            
        except Exception as e:
            print(f"  âŒ é”™è¯¯: {e}")
        
        print()

def test_concurrent_requests():
    """æµ‹è¯•å¹¶å‘è¯·æ±‚æ€§èƒ½"""
    print("ğŸ”€ å¹¶å‘è¯·æ±‚æµ‹è¯•...\n")
    
    import concurrent.futures
    import threading
    
    def make_request(request_id):
        """å‘èµ·å•ä¸ªè¯·æ±‚"""
        client = OpenAI(
            base_url="http://127.0.0.1:8000/v1",
            api_key="dummy"
        )
        
        start_time = time.time()
        try:
            response = client.chat.completions.create(
                model="Qwen/Qwen3-8B-AWQ",
                messages=[{"role": "user", "content": f"Hello, this is request {request_id}"}],
                max_tokens=30,
                temperature=0.5
            )
            end_time = time.time()
            return {
                "id": request_id,
                "success": True,
                "time": end_time - start_time,
                "tokens": response.usage.completion_tokens
            }
        except Exception as e:
            end_time = time.time()
            return {
                "id": request_id,
                "success": False,
                "time": end_time - start_time,
                "error": str(e)
            }
    
    # æµ‹è¯•ä¸åŒçš„å¹¶å‘çº§åˆ«
    for concurrent_requests in [2, 3, 5]:
        print(f"ğŸ“Š æµ‹è¯• {concurrent_requests} ä¸ªå¹¶å‘è¯·æ±‚:")
        
        start_time = time.time()
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_requests) as executor:
            futures = [executor.submit(make_request, i) for i in range(concurrent_requests)]
            results = [future.result() for future in concurrent.futures.as_completed(futures)]
        
        end_time = time.time()
        total_time = end_time - start_time
        
        successful = [r for r in results if r["success"]]
        failed = [r for r in results if not r["success"]]
        
        if successful:
            avg_time = sum(r["time"] for r in successful) / len(successful)
            total_tokens = sum(r["tokens"] for r in successful)
            throughput = total_tokens / total_time
            
            print(f"  âœ… æˆåŠŸ: {len(successful)}/{concurrent_requests}")
            print(f"  â±ï¸  æ€»æ—¶é—´: {total_time:.2f}s")
            print(f"  ğŸ“Š å¹³å‡å»¶è¿Ÿ: {avg_time:.2f}s")
            print(f"  ğŸ”¥ æ€»ååé‡: {throughput:.2f} tokens/s")
        
        if failed:
            print(f"  âŒ å¤±è´¥: {len(failed)} ä¸ªè¯·æ±‚")
        
        print()

def main():
    print("ğŸ§ª TransformersOpenAI API æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # ç¡®ä¿æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ
    try:
        response = requests.get("http://127.0.0.1:8000/health")
        print(f"âœ… æœåŠ¡å™¨çŠ¶æ€: {response.json()}\n")
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨: {e}")
        return
    
    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    test_non_streaming_performance()
    test_streaming_performance() 
    test_concurrent_requests()
    
    print("âœ… æ‰€æœ‰æ€§èƒ½æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    main()

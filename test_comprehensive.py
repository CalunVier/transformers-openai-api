#!/usr/bin/env python3
"""
å…¨é¢çš„ API æ€§èƒ½å’ŒåŠŸèƒ½æµ‹è¯•
"""
import asyncio
import time
import json
from concurrent.futures import ThreadPoolExecutor
import requests
from openai import OpenAI

# API é…ç½®
BASE_URL = "http://127.0.0.1:8000"
MODEL_NAME = "Qwen/Qwen3-8B-AWQ"

def test_basic_functionality():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    print("ğŸ”§ Testing basic functionality...")
    
    # å¥åº·æ£€æŸ¥
    response = requests.get(f"{BASE_URL}/health")
    assert response.status_code == 200
    health_data = response.json()
    print(f"âœ… Health check: {health_data}")
    
    # æ¨¡å‹åˆ—è¡¨
    response = requests.get(f"{BASE_URL}/v1/models")
    assert response.status_code == 200
    models_data = response.json()
    print(f"âœ… Models: {len(models_data['data'])} models available")

def test_chat_completion():
    """æµ‹è¯•èŠå¤©å®Œæˆ"""
    print("\nğŸ’¬ Testing chat completion...")
    
    client = OpenAI(base_url=f"{BASE_URL}/v1", api_key="dummy")
    
    # ç®€å•å¯¹è¯
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[{"role": "user", "content": "What is 2+2?"}],
        max_tokens=50,
        temperature=0.1
    )
    
    print(f"âœ… Math question: {response.choices[0].message.content.strip()}")
    
    # å¤šè½®å¯¹è¯
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {"role": "user", "content": "My name is Alice"},
            {"role": "assistant", "content": "Nice to meet you, Alice!"},
            {"role": "user", "content": "What's my name?"}
        ],
        max_tokens=30,
        temperature=0.1
    )
    
    print(f"âœ… Multi-turn: {response.choices[0].message.content.strip()}")

def test_streaming():
    """æµ‹è¯•æµå¼å“åº”"""
    print("\nğŸŒŠ Testing streaming...")
    
    client = OpenAI(base_url=f"{BASE_URL}/v1", api_key="dummy")
    
    start_time = time.time()
    content = ""
    
    stream = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[{"role": "user", "content": "Count from 1 to 3"}],
        max_tokens=50,
        temperature=0.1,
        stream=True
    )
    
    for chunk in stream:
        if chunk.choices[0].delta.content:
            content += chunk.choices[0].delta.content
    
    duration = time.time() - start_time
    print(f"âœ… Streaming response: {content.strip()}")
    print(f"â±ï¸ Streaming duration: {duration:.2f}s")

def test_concurrent_requests():
    """æµ‹è¯•å¹¶å‘è¯·æ±‚"""
    print("\nğŸš€ Testing concurrent requests...")
    
    def make_request(i):
        client = OpenAI(base_url=f"{BASE_URL}/v1", api_key="dummy")
        try:
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": f"Generate a random number for request {i}"}],
                max_tokens=20,
                temperature=0.8
            )
            return {"success": True, "request_id": i, "response": response.choices[0].message.content.strip()}
        except Exception as e:
            return {"success": False, "request_id": i, "error": str(e)}
    
    start_time = time.time()
    
    # å¹¶å‘ 5 ä¸ªè¯·æ±‚
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(make_request, i) for i in range(5)]
        results = [future.result() for future in futures]
    
    duration = time.time() - start_time
    successful = sum(1 for r in results if r["success"])
    
    print(f"âœ… Concurrent requests: {successful}/5 successful")
    print(f"â±ï¸ Total duration: {duration:.2f}s")
    
    for result in results:
        if result["success"]:
            print(f"  Request {result['request_id']}: {result['response']}")
        else:
            print(f"  Request {result['request_id']}: ERROR - {result['error']}")

def test_parameter_variations():
    """æµ‹è¯•ä¸åŒå‚æ•°è®¾ç½®"""
    print("\nğŸ›ï¸ Testing parameter variations...")
    
    client = OpenAI(base_url=f"{BASE_URL}/v1", api_key="dummy")
    
    # æµ‹è¯•ä¸åŒæ¸©åº¦
    for temp in [0.1, 0.5, 0.9]:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": "Say hello creatively"}],
            max_tokens=30,
            temperature=temp
        )
        print(f"âœ… Temperature {temp}: {response.choices[0].message.content.strip()}")
    
    # æµ‹è¯•ä¸åŒ max_tokens
    for max_tokens in [10, 50, 100]:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": "Tell me about artificial intelligence"}],
            max_tokens=max_tokens,
            temperature=0.7
        )
        print(f"âœ… Max tokens {max_tokens}: {len(response.choices[0].message.content)} chars")

def benchmark_performance():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\nğŸ“Š Performance benchmark...")
    
    client = OpenAI(base_url=f"{BASE_URL}/v1", api_key="dummy")
    
    # å•ä¸ªè¯·æ±‚æ€§èƒ½
    start_time = time.time()
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[{"role": "user", "content": "Write a short poem about technology"}],
        max_tokens=100,
        temperature=0.7
    )
    duration = time.time() - start_time
    
    tokens = response.usage.total_tokens
    tokens_per_second = tokens / duration
    
    print(f"âœ… Single request performance:")
    print(f"  Duration: {duration:.2f}s")
    print(f"  Total tokens: {tokens}")
    print(f"  Tokens/second: {tokens_per_second:.2f}")
    print(f"  Response: {response.choices[0].message.content.strip()[:100]}...")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª Starting comprehensive API testing...\n")
    
    try:
        test_basic_functionality()
        test_chat_completion()
        test_streaming()
        test_concurrent_requests()
        test_parameter_variations()
        benchmark_performance()
        
        print("\nâœ… All tests completed successfully!")
        
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

# Transformers OpenAI API

æœ¬ç¨‹åºç”¨ä¸ºç”± Transformers è¿è¡Œçš„æ¨¡å‹æä¾›ä¸€ä¸ª OpenAI å…¼å®¹çš„ API æ¥å£ã€‚ä½¿ç”¨ FastAPI æ„å»ºï¼Œæ”¯æŒæµå¼å’Œéæµå¼å“åº”ï¼Œå®Œå…¨å…¼å®¹ OpenAI Chat Completions APIã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸš€ OpenAI Chat Completions API å…¼å®¹
- ğŸ“¡ æ”¯æŒæµå¼å’Œéæµå¼å“åº”
- ğŸ”§ åŸºäº Transformers åº“çš„æ¨¡å‹æ¨ç†
- âš¡ æ”¯æŒ CUDA åŠ é€Ÿå’Œä¼˜åŒ–
- ğŸ›ï¸ å¯é…ç½®çš„å‚æ•°ï¼ˆæ¸©åº¦ã€top_pã€max_tokens ç­‰ï¼‰
- ğŸ“Š æ”¯æŒæ‰¹å¤„ç†å’Œå¹¶å‘é™åˆ¶
- ğŸ³ Docker æ”¯æŒ
- ğŸ” å¥åº·æ£€æŸ¥å’Œç›‘æ§ç«¯ç‚¹

## å¿«é€Ÿå¼€å§‹

### æ–¹æ³• 1: ä½¿ç”¨ PowerShell è„šæœ¬ï¼ˆæ¨èï¼‰

```powershell
# ç›´æ¥è¿è¡Œå¯åŠ¨è„šæœ¬
.\start.ps1
```

### æ–¹æ³• 2: æ‰‹åŠ¨å®‰è£…

```bash
# 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv

# 2. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ (Windows)
venv\Scripts\activate

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 4. å¯åŠ¨æœåŠ¡å™¨
python main.py
```

### æ–¹æ³• 3: ä½¿ç”¨ Docker

```bash
# æ„å»ºå¹¶è¿è¡Œ
docker-compose up --build
```

## API ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬èŠå¤©å®Œæˆ

```python
import requests
import json

url = "http://localhost:7088/v1/chat/completions"
payload = {
    "model": "mesolitica/malaysian-llama2-7b-32k-instructions",
    "messages": [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello! How are you?"}
    ],
    "max_tokens": 100,
    "temperature": 0.7
}

response = requests.post(url, json=payload)
print(response.json())
```

### æµå¼å“åº”

```python
import requests
import json

url = "http://localhost:7088/v1/chat/completions"
payload = {
    "model": "mesolitica/malaysian-llama2-7b-32k-instructions",
    "messages": [
        {"role": "user", "content": "Tell me a story"}
    ],
    "max_tokens": 200,
    "temperature": 0.8,
    "stream": True
}

response = requests.post(url, json=payload, stream=True)
for line in response.iter_lines():
    if line:
        line = line.decode('utf-8')
        if line.startswith('data: '):
            data = line[6:]
            if data.strip() == '[DONE]':
                break
            try:
                chunk = json.loads(data)
                if chunk['choices'][0]['delta'].get('content'):
                    print(chunk['choices'][0]['delta']['content'], end='', flush=True)
            except json.JSONDecodeError:
                continue
```

### è·å–å¯ç”¨æ¨¡å‹

```python
import requests

response = requests.get("http://localhost:7088/v1/models")
print(response.json())
```

## æµ‹è¯•å®¢æˆ·ç«¯

è¿è¡ŒåŒ…å«çš„æµ‹è¯•å®¢æˆ·ç«¯ï¼š

```bash
python example_client.py
```

## API ç«¯ç‚¹

- `GET /v1/models` - åˆ—å‡ºå¯ç”¨æ¨¡å‹
- `POST /v1/chat/completions` - åˆ›å»ºèŠå¤©å®Œæˆ
- `GET /health` - å¥åº·æ£€æŸ¥
- `GET /` - æ ¹ç«¯ç‚¹ä¿¡æ¯

```
usage: main.py [-h] [--host HOST] [--port PORT] [--loglevel LOGLEVEL] [--model-type MODEL_TYPE]
               [--tokenizer-type TOKENIZER_TYPE] [--tokenizer-use-fast TOKENIZER_USE_FAST] [--processor-type PROCESSOR_TYPE]
               [--hf-model HF_MODEL] [--torch-dtype TORCH_DTYPE] [--architecture-type {decoder,encoder-decoder}]
               [--serving-type {chat,whisper}] [--continuous-batching-microsleep CONTINUOUS_BATCHING_MICROSLEEP]
               [--continuous-batching-batch-size CONTINUOUS_BATCHING_BATCH_SIZE] [--static-cache STATIC_CACHE]
               [--static-cache-encoder-max-length STATIC_CACHE_ENCODER_MAX_LENGTH]
               [--static-cache-decoder-max-length STATIC_CACHE_DECODER_MAX_LENGTH] [--accelerator-type ACCELERATOR_TYPE]
               [--max-concurrent MAX_CONCURRENT] [--torch-profiling TORCH_PROFILING] [--hqq HQQ]
               [--torch-compile TORCH_COMPILE] [--torch-compile-mode TORCH_COMPILE_MODE]

Configuration parser

options:
  -h, --help            show this help message and exit
  --host HOST           host name to host the app (default: 0.0.0.0, env: HOSTNAME)
  --port PORT           port to host the app (default: 7088, env: PORT)
  --loglevel LOGLEVEL   Logging level (default: INFO, env: LOGLEVEL)
  --model-type MODEL_TYPE
                        Model type (default: AutoModelForCausalLM, env: MODEL_TYPE)
  --tokenizer-type TOKENIZER_TYPE
                        Tokenizer type (default: AutoTokenizer, env: TOKENIZER_TYPE)
  --tokenizer-use-fast TOKENIZER_USE_FAST
                        Use fast tokenizer (default: True, env: TOKENIZER_USE_FAST)
  --processor-type PROCESSOR_TYPE
                        Processor type (default: AutoTokenizer, env: PROCESSOR_TYPE)
  --hf-model HF_MODEL   Hugging Face model (default: mesolitica/malaysian-llama2-7b-32k-instructions, env: HF_MODEL)
  --torch-dtype TORCH_DTYPE
                        Torch data type (default: bfloat16, env: TORCH_DTYPE)
  --architecture-type {decoder,encoder-decoder}
                        Architecture type (default: decoder, env: ARCHITECTURE_TYPE)
  --serving-type {chat,whisper}
                        Serving type (default: chat, env: SERVING_TYPE)
  --continuous-batching-microsleep CONTINUOUS_BATCHING_MICROSLEEP
                        microsleep to group continuous batching, 1 / 1e-4 = 10k steps for one second (default: 0.001, env:
                        CONTINUOUS_BATCHING_MICROSLEEP)
  --continuous-batching-batch-size CONTINUOUS_BATCHING_BATCH_SIZE
                        maximum of batch size during continuous batching (default: 20, env: CONTINUOUS_BATCHING_BATCH_SIZE)
  --static-cache STATIC_CACHE
                        Preallocate KV Cache for faster inference (default: False, env: STATIC_CACHE)
  --static-cache-encoder-max-length STATIC_CACHE_ENCODER_MAX_LENGTH
                        Maximum concurrent requests (default: 256, env: STATIC_CACHE_ENCODER_MAX_LENGTH)
  --static-cache-decoder-max-length STATIC_CACHE_DECODER_MAX_LENGTH
                        Maximum concurrent requests (default: 256, env: STATIC_CACHE_DECODER_MAX_LENGTH)
  --accelerator-type ACCELERATOR_TYPE
                        Accelerator type (default: cuda, env: ACCELERATOR_TYPE)
  --max-concurrent MAX_CONCURRENT
                        Maximum concurrent requests (default: 100, env: MAX_CONCURRENT)
  --torch-profiling TORCH_PROFILING
                        Use torch.autograd.profiler.profile() to profile prefill and step (default: False, env:
                        TORCH_PROFILING)
  --hqq HQQ             int4 quantization using HQQ (default: False, env: HQQ)
  --torch-compile TORCH_COMPILE
                        Torch compile necessary forwards, can speed up at least 1.5X (default: False, env: TORCH_COMPILE)
  --torch-compile-mode TORCH_COMPILE_MODE
                        torch compile type (default: reduce-overhead, env: TORCH_COMPILE_MODE)
```

## é…ç½®å‚æ•°

æ‰€æœ‰é…ç½®å‚æ•°éƒ½å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æˆ–ç¯å¢ƒå˜é‡è®¾ç½®ï¼š

### åŸºæœ¬é…ç½®
- `--host` / `HOSTNAME`: æœåŠ¡å™¨ä¸»æœº (é»˜è®¤: 0.0.0.0)
- `--port` / `PORT`: æœåŠ¡å™¨ç«¯å£ (é»˜è®¤: 7088)
- `--loglevel` / `LOGLEVEL`: æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)

### æ¨¡å‹é…ç½®
- `--hf-model` / `HF_MODEL`: Hugging Face æ¨¡å‹åç§°
- `--model-type` / `MODEL_TYPE`: æ¨¡å‹ç±»å‹ (é»˜è®¤: AutoModelForCausalLM)
- `--tokenizer-type` / `TOKENIZER_TYPE`: åˆ†è¯å™¨ç±»å‹ (é»˜è®¤: AutoTokenizer)
- `--torch-dtype` / `TORCH_DTYPE`: æ•°æ®ç±»å‹ (é»˜è®¤: bfloat16)

### æ€§èƒ½ä¼˜åŒ–
- `--accelerator-type` / `ACCELERATOR_TYPE`: åŠ é€Ÿå™¨ç±»å‹ (é»˜è®¤: cuda)
- `--max-concurrent` / `MAX_CONCURRENT`: æœ€å¤§å¹¶å‘è¯·æ±‚æ•° (é»˜è®¤: 100)
- `--torch-compile` / `TORCH_COMPILE`: å¯ç”¨ Torch ç¼–è¯‘ä¼˜åŒ–
- `--static-cache` / `STATIC_CACHE`: é¢„åˆ†é… KV ç¼“å­˜

### æ‰¹å¤„ç†é…ç½®
- `--continuous-batching-batch-size`: è¿ç»­æ‰¹å¤„ç†çš„æœ€å¤§æ‰¹æ¬¡å¤§å° (é»˜è®¤: 20)
- `--continuous-batching-microsleep`: æ‰¹å¤„ç†å¾®ç¡çœ æ—¶é—´ (é»˜è®¤: 0.001)

### ç¤ºä¾‹å¯åŠ¨å‘½ä»¤

```bash
python main.py \
    --host 0.0.0.0 \
    --port 7088 \
    --hf-model microsoft/DialoGPT-medium \
    --torch-dtype float16 \
    --max-concurrent 50 \
    --torch-compile True
```

## é¡¹ç›®ç»“æ„

```
transformers-openai-api/
â”œâ”€â”€ main.py              # åº”ç”¨å…¥å£ç‚¹
â”œâ”€â”€ app.py               # FastAPI åº”ç”¨
â”œâ”€â”€ config.py            # é…ç½®ç®¡ç†
â”œâ”€â”€ models.py            # Pydantic æ•°æ®æ¨¡å‹
â”œâ”€â”€ model_manager.py     # æ¨¡å‹ç®¡ç†å™¨
â”œâ”€â”€ example_client.py    # æµ‹è¯•å®¢æˆ·ç«¯
â”œâ”€â”€ requirements.txt     # Python ä¾èµ–
â”œâ”€â”€ Dockerfile           # Docker é…ç½®
â”œâ”€â”€ docker-compose.yml   # Docker Compose é…ç½®
â”œâ”€â”€ start.ps1           # Windows å¯åŠ¨è„šæœ¬
â””â”€â”€ README.md           # é¡¹ç›®æ–‡æ¡£
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **CUDA å†…å­˜ä¸è¶³**
   - å°è¯•ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹
   - è®¾ç½® `--torch-dtype float16` æˆ– `--hqq True`
   - å‡å°‘ `--max-concurrent` å€¼

2. **æ¨¡å‹åŠ è½½æ…¢**
   - ç¡®ä¿ç½‘ç»œè¿æ¥ç¨³å®š
   - è€ƒè™‘ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„

3. **API å“åº”æ…¢**
   - å¯ç”¨ `--torch-compile True`
   - ä½¿ç”¨ `--static-cache True`
   - è°ƒæ•´ `--continuous-batching-batch-size`

### æ—¥å¿—

å¯ç”¨è¯¦ç»†æ—¥å¿—ï¼š
```bash
python main.py --loglevel DEBUG
```

## å…¼å®¹æ€§

æœ¬ API å…¼å®¹ OpenAI Chat Completions APIï¼Œå¯ä»¥ç›´æ¥æ›¿æ¢ OpenAI çš„ç«¯ç‚¹ä½¿ç”¨ã€‚æ”¯æŒçš„å‚æ•°ï¼š

- `model`: æ¨¡å‹åç§°
- `messages`: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
- `max_tokens`: æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°
- `temperature`: é‡‡æ ·æ¸©åº¦
- `top_p`: Top-p é‡‡æ ·
- `stream`: æ˜¯å¦æµå¼å“åº”
- `stop`: åœæ­¢åºåˆ—

## è®¸å¯è¯

MIT License

## è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼